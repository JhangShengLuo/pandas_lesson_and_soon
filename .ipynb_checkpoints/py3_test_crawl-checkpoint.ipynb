{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "import operator\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cookies=pd.read_csv(\"cookierunzh.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(line):\n",
    "    rule = re.compile(r\"[^a-zA-Z0-9\\u4e00-\\u9fa5]\")\n",
    "    line = rule.sub('',line)\n",
    "    return line\n",
    "def draw_word_cloud( freq_list , file_name ):\n",
    "    # Generate a word cloud image\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='/Users/jackie/Desktop/NotoSansCJKtc-Regular.otf',\n",
    "        width=960,\n",
    "        height=480,\n",
    "        relative_scaling=.5\n",
    "    ).generate_from_frequencies( freq_list )\n",
    "    wordcloud.to_file( file_name )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/jackie/Desktop/dict.txt.big ...\n",
      "Loading model from cache /var/folders/bv/kbbg1cs90nv_zxrlscs0zx0w0000gn/T/jieba.u64beae7bf5ef2ca9f1784b18c571a0f2.cache\n",
      "Loading model cost 2.142 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "text_y=cookies.ix[:,\"message\"]\n",
    "\n",
    "sta=\"\"\n",
    "for sts in text_y:\n",
    "    sta+=str(sts)\n",
    "\n",
    "# 去除所有半角全角符号，只留字母、数字、中文。\n",
    "text_y=remove_punctuation(sta)\n",
    "\n",
    "jieba.set_dictionary(\"/Users/jackie/Desktop/dict.txt.big\")\n",
    "\n",
    "\n",
    "output = open('w2v_test.txt','w')\n",
    "words = jieba.cut(text_y, cut_all=False)\n",
    "for word in words:\n",
    "    output.write(word +' ')\n",
    "# 跑word2vec\n",
    "word_source=\"w2v_test.txt\"\n",
    "sentences = word2vec.Text8Corpus(word_source)\n",
    "model = word2vec.Word2Vec(sentences, size=250)\n",
    "\n",
    "model.most_similar(\"人\")\n",
    "# #保存模型，供日後使用\n",
    "# model.save(\"med250.model.bin\")\n",
    "\n",
    "# #模型讀取方式\n",
    "# # model = word2vec.Word2Vec.load(\"your_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/jackie/Desktop/dict.txt.big ...\n",
      "Loading model from cache /var/folders/bv/kbbg1cs90nv_zxrlscs0zx0w0000gn/T/jieba.u64beae7bf5ef2ca9f1784b18c571a0f2.cache\n",
      "Loading model cost 2.190 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "text_y=cookies.ix[:,\"message\"]\n",
    "\n",
    "sta=\"\"\n",
    "for sts in text_y:\n",
    "    sta+=str(sts)\n",
    "\n",
    "# 去除所有半角全角符号，只留字母、数字、中文。\n",
    "\n",
    "\n",
    "text_y=remove_punctuation(sta)\n",
    "\n",
    "jieba.set_dictionary(\"/Users/jackie/Desktop/dict.txt.big\")\n",
    "word_count={}\n",
    "for i in jieba.cut(text_y,cut_all=False):\n",
    "    if i in word_count:\n",
    "        word_count[i]+=1\n",
    "    else:\n",
    "        word_count[i]=1\n",
    "        \n",
    "sortrd_count = sorted(word_count.items(),key=operator.itemgetter(1),reverse=True)\n",
    "pd.DataFrame(sortrd_count).to_csv(\"word_count.csv\",index=False)\n",
    "\n",
    "draw_word_cloud( word_count ,'word_count.png' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fg=[1,1,1,1,1,2,2,2,2,2,3,3,3,3,4,44,4,4]\n",
    "fgfg=[2,1,3,1,1,4,2,2,2,2,2,3,3,3,3,4,44,4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1),\n",
       " (1, 2),\n",
       " (1, 3),\n",
       " (2, 2),\n",
       " (2, 4),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (4, 44),\n",
       " (44, 4)}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(zip(fg,fgfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gh=\"I am %s\"%(\"Groot\")\n",
    "hg = \"I am {}\".format(\"Groot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Groot'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method cut in module jieba:\n",
      "\n",
      "cut(sentence, cut_all=False, HMM=True) method of jieba.Tokenizer instance\n",
      "    The main function that segments an entire sentence that contains\n",
      "    Chinese characters into seperated words.\n",
      "    \n",
      "    Parameter:\n",
      "        - sentence: The str(unicode) to be segmented.\n",
      "        - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "        - HMM: Whether to use the Hidden Markov Model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(jieba.cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
